services:
  db:
    build: ./db
    environment:
      # src: https://stackoverflow.com/a/26599273/14751074
      - POSTGRES_USER=api_handler
      - POSTGRES_PASSWORD=temp # TODO: security risk
      - POSTGRES_DB=news_briefer # TODO: ugly names. change later
    ports:
      - 5432:5432
    volumes:
      - db_vol:/var/lib/postgresql/data
  client:
    build: ./client
    ports:
      - 3000:3000
  
  server:
    build: ./server
    environment:
      - LLM_SERVER=llm:11434
      - TTS_SERVER=tts:5002
      - JWT_SECRET_KEY=MyRandomSecretKey!
      - DB_USERNAME=bruh
      - DB_PASSWORD=bruh
      - MODEL_NAME=llama3.2
    ports:
      - 5000:5000

  tts:
    image: ghcr.io/coqui-ai/tts
    # expose these ports for debugging
    # ports:
    #   - 5002:5002
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    entrypoint: ["python3", "TTS/server/server.py", "--model_name", "tts_models/en/vctk/vits", "--use_cuda", "true"]

  # equivalent of `docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama`
  # https://stackoverflow.com/questions/78500319/how-to-pull-model-automatically-with-container-creation
  # https://github.com/valiantlynx/ollama-docker/blob/main/docker-compose-ollama-gpu.yaml
  llm:
    image: ollama/ollama:latest 
    # expose these ports for debugging
    # ports: 
    #   - 11434:11434
    volumes:
      - llm_vol:/root/.ollama
      - ./llm/entrypoint.sh:/entrypoint.sh
    environment:
      - MODEL_NAME=llama3.2
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # removing this thew everything back to CPU...
              capabilities: [gpu]
    entrypoint: ["/usr/bin/bash", "/entrypoint.sh"]
    
volumes:
  db_vol:
  llm_vol:
